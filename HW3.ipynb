{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3-Solutions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JumpLusWu/CIS700/blob/master/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TcAvpMNhZ6c6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Clone Git\n",
        "\n",
        "Run the following to get the required files needed for this assignment. \n",
        "\n",
        "TODO: switch repos"
      ]
    },
    {
      "metadata": {
        "id": "L-rDRgbYlvfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cis700/hw3-solutions.git\n",
        "!mv hw3-solutions/* .\n",
        "!rm -rf hw3-solutions/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FCEowgppabSx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 1.  Setting up the Reddit dataset"
      ]
    },
    {
      "metadata": {
        "id": "AKY6gdpKbT_M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For this assignment, we are going to use the Reddit sarcasm dataset.  Since sarcasm is difficult to express via text, Redditors frequently end sarcastic comments with \"/s\".  The Reddit dataset uses the presence of this \"/s\" token to create a labeled dataset of sarcastic and non-sarcastic comments.  The original Reddit dataset can be found here: https://www.kaggle.com/danofer/sarcasm\n",
        "\n",
        "We've made some slight modifications to the dataset -- we've removed metadata and balanced the dataset (only 1% of Reddit comments are actually sarcastic, but 50% of the training and test examples are sarcastic in the modified dataset).\n",
        "\n",
        "Note that even within the niche of sarcasm-based NLP, there are better, cleaner, and larger datasets than this Reddit dataset.  I've selected this one specifically because there are many teachable characteristics of the dataset.\n",
        "\n",
        "Run the following commands to unzip the csv file."
      ]
    },
    {
      "metadata": {
        "id": "8_1HNNeZaiIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gzip -d ./data/reddit_train.csv.gz\n",
        "!gzip -d ./data/reddit_test.csv.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5mQQ_Ux2bf4y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following cells have all of the necessary imports for this homework.  We see 2 new packages.\n",
        "\n",
        "\n",
        "1.   **torchtext**.  Recall that torchvision built many image datasets, CV models, and image processing utilities into the PyTorch framework.  The torchtext package does the same for NLP -- it has many utilities for handling tokenization, variable-length sequences, feature vectorization.\n",
        "2.   **spaCy**.  This is a state-of-the-art English language tokenizer.  We will pass this as an input to the torchtext equivalent of a DataLoader.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "M13DP7Jvbeuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ovS_g7dvccrC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import torchtext.data as data\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import random\n",
        "import time  # TODO: remove\n",
        "from google.colab import drive\n",
        "from helper import Logger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V3JasABrurXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# reset all environment conditions\n",
        "\n",
        "def reset_env():\n",
        "    SEED = 1234\n",
        "    random.seed(SEED)\n",
        "    torch.manual_seed(SEED)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "device =  torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NYUdd2ECdCjS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1.1 Review of the dataset\n",
        "\n",
        "Let's first understand the structure of the dataset.\n",
        "\n",
        "*   Print out the headers and the first five rows of reddit_train.csv.  Put this in your writeup.\n",
        "*   As a review of part 1, describe each input and state whether it is fixed-length or variable-length in your writeup.\n",
        "*   Print out size of both datasets and put the lengths in your writeup.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "T23DwRC5dZIC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reddit_train_df = pd.read_csv('./data/reddit_train.csv')\n",
        "reddit_test_df = pd.read_csv('./data/reddit_test.csv')\n",
        "print(reddit_train_df.head())\n",
        "\n",
        "print(len(reddit_train_df.index))\n",
        "print(len(reddit_test_df.index))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wcmwXGEfPQNW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Q1.2 Featurizing the dataset with torchtext\n",
        "\n",
        "Computer vision has torchvision; NLP has torchtext.  In this problem, we will create and featurize a torchtext dataset with Fields, a data structure that can automatically featurize text with embeddings.\n",
        "\n",
        "First, create a tokenizer using spacy_en.  Create two torchtext data fields using data.Field from torchtext.data:\n",
        "\n",
        "*   a sequential field named TEXT for comment and parent_comment.  (*Hint: since this is natural language, this is sequential data.  Use your tokenizer and convert all characters to lowercase.*)\n",
        "*   a non-sequential field named LABEL for the labels.  (*Hint: since this is a categorical variable, it is not sequential.  Furthermore, it does not require a vocabulary since there are no words to embed.*)\n",
        "\n",
        "Next, look at the documentation for data.TabularDataset.splits and create `train_ds` and `test_ds`.  You will need the paths to the training and test datasets, the format, and the mapping from columns to fields.  Note that the first column in the csv's should not be a field.  This step creates 3 torchtext objects for each row in the dataset, so it will take some time (between 2-10 minutes).\n",
        "\n",
        "We then build our vocabulary with GloVE, a word embedding similar to Word2Vec.  Create a vocabulary from TEXT using the train dataset (*Hint: look at the documentation for Field.build_vocab*).  Use the glove.6B.100d word embedding, and save the vocbulary.  The first time you run this, it will take roughly 5-10 minutes to download.  The pretrained model will then be stored in the ./.vector_cache folder, and rerunning this command will take negligible time.  Store the final vocabulary in the `vocab` variable.\n",
        "\n",
        "Finally, print out a single example from `train_ds` and print out the properties of `vocab` ('freqs', 'itos', 'stoi', 'vectors').  Include this printouts in the writeup.  Explain what the properties of `vocab` are.  These are the only things you need to include in your writeup for Q1.1b."
      ]
    },
    {
      "metadata": {
        "id": "sBPNdzZJhZmY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "spacy_en = spacy.load('en')\n",
        "spacy.prefer_gpu()\n",
        "\n",
        "def tokenizer(text):  # create a tokenizer function\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
        "LABEL = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "train_ds, test_ds = data.TabularDataset.splits(\n",
        "    path='./data/', train='reddit_train.csv',\n",
        "    test='reddit_test.csv', format='csv', skip_header=True,\n",
        "    fields=[('index', None), ('label', LABEL), ('comment', TEXT), ('parent_comment', TEXT)])\n",
        "\n",
        "TEXT.build_vocab(train_ds, vectors=\"glove.6B.100d\")\n",
        "vocab = TEXT.vocab\n",
        "\n",
        "print(vocab.__dict__.keys())\n",
        "# vocab.freqs\n",
        "# vocab.itos\n",
        "# vocab.stoi\n",
        "# vocab.vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zejlNkZbPl9m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility cells for modeling\n",
        "\n",
        "We've set up a few utility cells here.  Use these as you see fit.\n",
        "\n",
        "1.   Cell to set up a logger and Tensorboard.\n",
        "2.   Cell to keep track of hyperparameters.\n",
        "3.   Cell with functions for training and testing loops.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ALbxEo4-Pk89",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Tensorboard setup\n",
        "# asdf\n",
        "LOG_DIR = './logs'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "!if [ -f ngrok ] ; then echo \"Ngrok already installed\" ; else wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1 && unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1 ; fi\n",
        "\n",
        "!./ngrok authtoken 7rHP2EU3WwBpFXGh7cH3z_6VS67KiDzaRyVAKTLt8St\n",
        "    \n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print('Tensorboard Link: ' +str(json.load(sys.stdin)['tunnels'][0]['public_url']))\"\n",
        "\n",
        "logger = Logger('./logs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8X8W3AdB5vhY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### hyperparameters\n",
        "# overall\n",
        "all_models_hyperparameters = {'embedding_dim': 100,\n",
        "                              'output_dim': 1,\n",
        "                              'vocabulary_size': len(TEXT.vocab),\n",
        "                              'train_batch_size': 40,\n",
        "                              'test_batch_size': 1000}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxi8SQszDeQR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def process_example(m, ex, variable_length):\n",
        "    lab, child, parent = ex.label.to(device), ex.comment.to(device), ex.parent_comment.to(device)\n",
        "    if variable_length:\n",
        "        lengths = [list(c.size())[0] for c in child.permute(1, 0)]\n",
        "        out = torch.squeeze(m(child, parent, torch.LongTensor(lengths).cpu()), 1)\n",
        "    else:\n",
        "        out = torch.squeeze(m(child, parent), 1)\n",
        "    return lab, out\n",
        "\n",
        "\n",
        "def train_model(model_name, model, optimizer, loss_criterion, num_epochs, variable_length=False, parents=True):\n",
        "    tick = time.time()\n",
        "    # make sure model and loss are on CUDA\n",
        "    model = model.to(device)\n",
        "    loss_criterion = loss_criterion.to(device)\n",
        "\n",
        "    logger = Logger('./logs/' + model_name + str(time.time()))\n",
        "    batch_num = 0\n",
        "    max_accuracy = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"starting epoch \", epoch, \", \", time.time() - tick)\n",
        "        for example in train_iter:\n",
        "            batch_num += 1\n",
        "            label, output = process_example(model, example, variable_length)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_criterion(output.float(), label.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            ## Tensorboard stuff\n",
        "            \n",
        "            # computing train accuracy\n",
        "            predicted = torch.round(output.data)\n",
        "            total = label.size(0)\n",
        "            correct = (predicted.float() == label.to(device).float()).sum().item()\n",
        "            accuracy = correct / total\n",
        "            info = { 'loss': loss, 'accuracy': accuracy }\n",
        "            \n",
        "            # computing test accuracy\n",
        "            if batch_num % 20000 == 0:\n",
        "                test_total = 0\n",
        "                test_correct = 0\n",
        "                with torch.no_grad():\n",
        "                    for test_example in test_iter:\n",
        "                        test_label, test_output = process_example(model, test_example, variable_length)\n",
        "                        test_predicted = torch.round(test_output.data)\n",
        "                        test_total += test_label.size(0)\n",
        "                        test_correct += (test_predicted.float() == test_label.to(device).float()).sum().item()\n",
        "                        break;  # only takes one test batch\n",
        "                test_accuracy = test_correct / test_total\n",
        "                info['test_accuracy'] = test_accuracy                  \n",
        "                if test_accuracy > max_accuracy:\n",
        "                    torch.save(model.state_dict(), \"/content/gdrive/My Drive/hw-3-models/\" + model_name + \"-\" + str(batch_num))\n",
        "\n",
        "            for tag, value in info.items():\n",
        "                logger.scalar_summary(tag, value, batch_num + 1)        \n",
        "    return model\n",
        "\n",
        "def test_model_accuracy(model, variable_length=False):\n",
        "    model = model.to(device)\n",
        "    confusion_mtx = np.zeros((2, 2))\n",
        "    test_total = 0\n",
        "    test_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for test_example in test_iter:\n",
        "            test_label, test_output = process_example(model, test_example, variable_length)\n",
        "            test_predicted = torch.round(test_output.data)\n",
        "            test_total += test_label.size(0)\n",
        "            test_correct += (test_predicted.float() == test_label.to(device).float()).sum().item()\n",
        "#             print(test_correct / test_total)\n",
        "    test_accuracy = test_correct / test_total\n",
        "    return test_accuracy\n",
        "\n",
        "def test_model_confusion_matrix(model, variable_length=False):\n",
        "    model = model.to(device)\n",
        "    confusion_mtx = np.zeros((2, 2))\n",
        "    with torch.no_grad():\n",
        "        for test_example in test_iter:\n",
        "            test_label = test_example.label.to(device)\n",
        "            test_child = test_example.comment.to(device)\n",
        "            test_parent = test_example.parent_comment.to(device)\n",
        "            \n",
        "            if test_child.size()[0] > 4:  # ensures we're looking at sufficiently large comments\n",
        "                if variable_length:\n",
        "                    lengths = [list(c.size())[0] for c in test_child.permute(1, 0)]\n",
        "#                     output = torch.squeeze(model(comment, torch.LongTensor(lengths).cpu()), 1)\n",
        "                    test_output = torch.squeeze(model(test_child, test_parent, torch.LongTensor(lengths).to(device)), 1)\n",
        "\n",
        "                else:\n",
        "                    test_output = torch.squeeze(model(test_child, test_parent), 1)\n",
        "                test_predicted = torch.round(test_output.data)\n",
        "                current_cm = confusion_matrix(test_label.cpu().numpy(), test_predicted.cpu().numpy())\n",
        "                confusion_mtx += current_cm\n",
        "    tn, fp, fn, tp = confusion_mtx.ravel()\n",
        "    accuracy = (tn + tp) / (tn + fp + fn + tp)\n",
        "    recall = tp / (tp + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"TPR / Recall / Sensitivity: \", recall)\n",
        "    print(\"Precision: \", precision)\n",
        "    print(\"F1: \", 2 * (precision * recall) / (precision + recall))\n",
        "    return tn, fp, fn, tp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c2-_bPaflAw6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = data.Iterator.splits(\n",
        "    (train_ds, test_ds), sort_key=lambda x: len(x.comment), shuffle=True,\n",
        "    batch_sizes=(all_models_hyperparameters['train_batch_size'], all_models_hyperparameters['test_batch_size']), device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AA5I1F-XSdKK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 2.  Modeling sarcasm without context"
      ]
    },
    {
      "metadata": {
        "id": "aXVln6-xQm9-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 2.1 Logistic Regression Model"
      ]
    },
    {
      "metadata": {
        "id": "ogzdgm3gQmQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "        self.embed.weight.data.copy_(vocab.vectors)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, text, parent=None):\n",
        "        text = text.permute(1, 0)\n",
        "        embedded = self.embed(text)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        avg_embedded = torch.mean(embedded, dim=2)\n",
        "        return torch.sigmoid(self.fc(avg_embedded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "29xcYWYZLRZf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reset_env()\n",
        "# base lr\n",
        "# attempt 0: 0.001 // peaks at 7 epochs\n",
        "base_lr_hyperparameters = {'learning_rate': 0.01,\n",
        "                           'num_epochs': 10} # 80k examples}\n",
        "\n",
        "logistic_reg = LogisticRegression(all_models_hyperparameters['embedding_dim'], \n",
        "                                  all_models_hyperparameters['output_dim'])\n",
        "optimizer = optim.Adam(logistic_reg.parameters(), lr=base_lr_hyperparameters['learning_rate'])\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "logistic_reg = train_model('Logistic-Regression-attempt-0-', logistic_reg, optimizer, criterion, base_lr_hyperparameters['num_epochs'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nw58jcanEzGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# logistic_reg = LogisticRegression(all_models_hyperparameters['embedding_dim'], \n",
        "#                                   all_models_hyperparameters['output_dim'])\n",
        "# logistic_reg.load_state_dict(torch.load(\"/content/gdrive/My Drive/hw-3-models/Logistic-Regression-attempt-0--100000\"))\n",
        "\n",
        "test_model_accuracy(logistic_reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3IvJ-fpLlgIV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_model_confusion_matrix(logistic_reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vZBDpnhZQtwh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 2.2 CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "Ap1LzDCuh_I4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNN1d(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_features, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "        self.embed.weight.data.copy_(vocab.vectors)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_features,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_features, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, parent=None):\n",
        "        text = text.permute(1, 0)\n",
        "        embedded = self.embed(text)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "        return torch.sigmoid(self.fc(cat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "htix_t7GsV5k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# base CNN\n",
        "# attempt 0: 60 x [3,4,5] x 0.001, dropout=0.5 (good) 0.7981794259015601\n",
        "# attempt 1: 80 x [3,4,5] x 0.0005, dropout=0.2 (bad)\n",
        "# attempt 2: 60 x [3,4,5] x 0.001, dropout=0.2 (bad)\n",
        "# attempt 3: 80 x [2,3,4,5,6] x  0.0005 x dropout=0.5 (bad)\n",
        "# attempt 4: 80 x [1,2,3,4,5] x 0.0005 x 0.5 (good) 0.8047 train; 0.7211 test\n",
        "# attempt 5: 80 x [1,2,3,4,5] x 0.0001 x 0.5 dropout x 30 epochs (good) 0.83 train; 0.7022 test\n",
        "# attempt 6: 40 x [1,2,3,4,5] x 0.0001 x 0.6 dropout x 15 epochs (good) 0.79 train; 0.707 test\n",
        "# attempt 7: 200 x [1, 3, 5] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 8: 80 x [1, 1, 2, 2, 3, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 9: 200 x [1, 2, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 10: 200 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 11: 80 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 12: 20 x [3,4,5] x 0.001 x 0.5 dropout x 10 epochs\n",
        "# attempt 13: 40 x [3,4,5] x 0.001 x 0.5 dropout x 20 epochs\n",
        "# attempt 14: 20 x [3,4,5] x 0.001 x 0.5 dropout x 20 epochs\n",
        "# attempt 14: 20 x [3,4,5] x 0.001 x 0.5 dropout x 25 epochs x bs 40\n",
        "base_cnn_hyperparameters = {'num_features': 20,\n",
        "                            'filter_sizes': [3, 4, 5], #[2-6] didn't work\n",
        "                            'learning_rate': 0.001,\n",
        "                            'num_epochs': 25,\n",
        "                            'dropout': 0.2}\n",
        "cnn = CNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "            base_cnn_hyperparameters['num_features'],\n",
        "            base_cnn_hyperparameters['filter_sizes'],\n",
        "            all_models_hyperparameters['output_dim'],\n",
        "            base_cnn_hyperparameters['dropout']).to(device)\n",
        "\n",
        "optimizer = optim.Adam(cnn.parameters(), lr=base_cnn_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "cnn = train_model('CNN-attempt-14-', cnn, optimizer, criterion, base_cnn_hyperparameters['num_epochs'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A9dXDDHmqg44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# cnn = CNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "#             base_cnn_hyperparameters['num_features'],\n",
        "#             base_cnn_hyperparameters['filter_sizes'],\n",
        "#             all_models_hyperparameters['output_dim'],\n",
        "#             all_models_hyperparameters['dropout']).to(device)\n",
        "# cnn.load_state_dict(torch.load(\"/content/gdrive/My Drive/hw-3-models/CNN-attempt-12--240000\"))\n",
        "\n",
        "test_model_confusion_matrix(cnn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M4FUVYqWRQp3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 2.3 LSTM Model"
      ]
    },
    {
      "metadata": {
        "id": "wDGeFhBx28VU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, parent, text_lengths):\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return torch.sigmoid(self.fc(hidden.squeeze(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3cfX06Lxh_Qc",
        "colab_type": "code",
        "outputId": "15bcf04e-2bc3-42a1-efcf-d6992a01416f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "cell_type": "code",
      "source": [
        "# base RNN\n",
        "# attempt 0: 15 epochs, 256 hiddens, 2 layers, dropout = 0.2, lr = 0.0005\n",
        "# attempt 1: 15 epochs, 128 hiddens, 1 layers, dropout = 0.5, lr = 0.0001 (not quite as good)\n",
        "# attempt 2: 15 epochs, 256 hiddens, 1 layers, dropout = 0.2, lr = 0.001\n",
        "# attempt 3: 10 epochs, 50 hiddens, 1 layers, dropout = 0.5, lr = 0.001\n",
        "# attempt 4: 20 epochs, 100 hiddens, 1 layers, dropout = 0.5, lr = 0.001\n",
        "base_rnn_hyperparameters = {'hidden_size': 100,\n",
        "                            'number_of_layers': 1,\n",
        "                            'bidirectional': True,\n",
        "                            'dropout': 0.2,\n",
        "                            'pad_idx': TEXT.vocab.stoi[TEXT.pad_token],\n",
        "                            'learning_rate': 0.001,\n",
        "                            'num_epochs': 20}\n",
        "rnn = RNN(all_models_hyperparameters['vocabulary_size'],\n",
        "          all_models_hyperparameters['embedding_dim'], \n",
        "          base_rnn_hyperparameters['hidden_size'],\n",
        "          all_models_hyperparameters['output_dim'],\n",
        "          base_rnn_hyperparameters['number_of_layers'],\n",
        "          base_rnn_hyperparameters['bidirectional'],\n",
        "          all_models_hyperparameters['dropout'],\n",
        "          base_rnn_hyperparameters['pad_idx'])\n",
        "\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=base_rnn_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "rnn = train_model('RNN-attempt-0-', rnn, optimizer, criterion, base_rnn_hyperparameters['num_epochs'], variable_length=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-21cc2da52fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RNN-attempt-0-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: train_model() missing 1 required positional argument: 'num_epochs'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "j_TFcvkGh_TB",
        "colab_type": "code",
        "outputId": "c03edfde-a671-4719-dfbe-44b5b9ce80ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "test_model_confusion_matrix(rnn, variable_length=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.7053582535141186\n",
            "TPR / Recall / Sensitivity:  0.6214031878364397\n",
            "Precision:  0.766153334196077\n",
            "F1:  0.6862280803430975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98568.0, 25298.0, 50498.0, 82884.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "_aVjG6Oyh_VT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_FqT-CX5arho",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3.  Modeling sarcasm by concatenating context"
      ]
    },
    {
      "metadata": {
        "id": "foLndccIPWTq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Utility cells for modeling"
      ]
    },
    {
      "metadata": {
        "id": "0-elF8iXPQew",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### hyperparameters\n",
        "# overall\n",
        "all_models_hyperparameters = {'embedding_dim': 100,\n",
        "                              'output_dim': 1,\n",
        "                              'dropout': 0.5,\n",
        "                              'vocabulary_size': len(TEXT.vocab),\n",
        "                              'number_of_epochs': 10,\n",
        "                              'batch_size': 50}\n",
        "\n",
        "# base lr\n",
        "# attempt 0: 0.001\n",
        "two_utt_lr_hyperparameters = {'learning_rate': 0.001}\n",
        "\n",
        "# base CNN\n",
        "# attempt 0: 60 x [3,4,5] x 0.001, dropout=0.5 (good) 0.7981794259015601\n",
        "# attempt 1: 80 x [3,4,5] x 0.0005, dropout=0.2 (bad)\n",
        "# attempt 2: 60 x [3,4,5] x 0.001, dropout=0.2 (bad)\n",
        "# attempt 3: 80 x [2,3,4,5,6] x  0.0005 x dropout=0.5 (bad)\n",
        "# attempt 4: 80 x [1,2,3,4,5] x 0.0005 x 0.5 (good) 0.8047 train; 0.7211 test\n",
        "# attempt 5: 80 x [1,2,3,4,5] x 0.0001 x 0.5 dropout x 30 epochs (good) 0.83 train; 0.7022 test\n",
        "# attempt 6: 40 x [1,2,3,4,5] x 0.0001 x 0.6 dropout x 15 epochs (good) 0.79 train; 0.707 test\n",
        "# attempt 7: 200 x [1, 3, 5] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 8: 80 x [1, 1, 2, 2, 3, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 9: 200 x [1, 2, 3, 4, 5, 8, 10] x 0.0005 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 10: 200 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "# attempt 11: 80 x [1, 2, 3] x 0.0001 x 0.5 dropout x 15 epochs (bad)\n",
        "two_utt_cnn_hyperparameters = {'num_features': 200,\n",
        "                            'filter_sizes': [1, 2, 3], #[2-6] didn't work\n",
        "                            'learning_rate': 0.0002}\n",
        "\n",
        "# base RNN\n",
        "# attempt 0: 15 epochs, 256 hiddens, 2 layers, dropout = 0.2, lr = 0.0005\n",
        "# attempt 1: 15 epochs, 128 hiddens, 1 layers, dropout = 0.5, lr = 0.0001 (not quite as good)\n",
        "# attempt 2: 15 epochs, 256 hiddens, 1 layers, dropout = 0.2, lr = 0.001\n",
        "two_utt_rnn_hyperparameters = {'hidden_size': 256,\n",
        "                            'number_of_layers': 1,\n",
        "                            'bidirectional': True,\n",
        "                            'dropout': 0.2,\n",
        "                            'pad_idx': TEXT.vocab.stoi[TEXT.pad_token],\n",
        "                            'learning_rate': 0.0001}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HvRNbKln3f7-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 3.1 Logistic Regression Model"
      ]
    },
    {
      "metadata": {
        "id": "GK2NokW_wkxv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConcatenatedLogisticRegression(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "        self.embed.weight.data.copy_(vocab.vectors)\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, text, parent=None):\n",
        "        text = torch.cat((parent, text))\n",
        "        text = text.permute(1, 0)\n",
        "        embedded = self.embed(text)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        avg_embedded = torch.mean(embedded, dim=2)\n",
        "        return torch.sigmoid(self.fc(avg_embedded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V1dpXZn7bv3k",
        "colab_type": "code",
        "outputId": "9deb1f3f-4330-46d5-bf35-ead71a0f2493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "cell_type": "code",
      "source": [
        "tick = time.time()\n",
        "logistic_reg = ConcatenatedLogisticRegression(all_models_hyperparameters['embedding_dim'], \n",
        "                                  all_models_hyperparameters['output_dim'])\n",
        "\n",
        "optimizer = optim.Adam(logistic_reg.parameters(), lr=two_utt_lr_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "logistic_reg = train_model('cu-Logistic-Regression-attempt-0-', logistic_reg, optimizer, criterion)\n",
        "print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting epoch  0 ,  0.018880844116210938\n",
            "starting epoch  1 ,  571.3769717216492\n",
            "starting epoch  2 ,  1141.6802339553833\n",
            "starting epoch  3 ,  1713.1731312274933\n",
            "starting epoch  4 ,  2283.1990246772766\n",
            "starting epoch  5 ,  2854.5207936763763\n",
            "starting epoch  6 ,  3425.7746438980103\n",
            "starting epoch  7 ,  3996.043228626251\n",
            "starting epoch  8 ,  4567.509980916977\n",
            "starting epoch  9 ,  5141.003987073898\n",
            "starting epoch  10 ,  5715.620518922806\n",
            "starting epoch  11 ,  6289.74947810173\n",
            "starting epoch  12 ,  6865.116789340973\n",
            "starting epoch  13 ,  7440.568010091782\n",
            "starting epoch  14 ,  8015.557572841644\n",
            "8590.036380052567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gZ38S7AqbvzY",
        "colab_type": "code",
        "outputId": "c6bf7a16-14f3-4366-c877-205b24802c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "logistic_reg = ConcatenatedLogisticRegression(all_models_hyperparameters['embedding_dim'], \n",
        "                                  all_models_hyperparameters['output_dim'])\n",
        "logistic_reg.load_state_dict(torch.load(\"/content/gdrive/My Drive/cu-Logistic-Regression-attempt-0-\"))\n",
        "# print(\"done!\")\n",
        "\n",
        "test_model_confusion_matrix(logistic_reg, variable_length=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6449582940677108\n",
            "TPR / Recall / Sensitivity:  0.49675631842140827\n",
            "Precision:  0.7325766803233308\n",
            "F1:  0.592048037513647\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(99465.0, 24151.0, 67023.0, 66159.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "metadata": {
        "id": "HOlKlcnQ-HXb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 3.2 CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "Qce-Anie-Pld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConcatenatedCNN1d(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_features, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "        self.embed.weight.data.copy_(vocab.vectors)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_features,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * num_features, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, parent=None):\n",
        "        text = text.permute(1, 0)\n",
        "        parent = parent.permute(1, 0)\n",
        "        text = torch.cat((parent, text), dim=1)\n",
        "        embedded = self.embed(text)\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "        return torch.sigmoid(self.fc(cat))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J-mfEvUjoAvs",
        "colab_type": "code",
        "outputId": "0e79096e-94bc-44d5-efcc-a42ad1c8e86d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "tick = time.time()\n",
        "concat_cnn = ConcatenatedCNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "            two_utt_cnn_hyperparameters['num_features'],\n",
        "            two_utt_cnn_hyperparameters['filter_sizes'],\n",
        "            all_models_hyperparameters['output_dim'],\n",
        "            all_models_hyperparameters['dropout']).to(device)\n",
        "\n",
        "optimizer = optim.Adam(concat_cnn.parameters(), lr=two_utt_cnn_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "concat_cnn = train_model('concat-CNN-attempt-0-', concat_cnn, optimizer, criterion)\n",
        "print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting epoch  0 ,  0.0009129047393798828\n",
            "starting epoch  1 ,  843.1480114459991\n",
            "starting epoch  2 ,  1685.5176486968994\n",
            "starting epoch  3 ,  2528.366101503372\n",
            "starting epoch  4 ,  3370.2224912643433\n",
            "starting epoch  5 ,  4212.813880205154\n",
            "starting epoch  6 ,  5055.7967228889465\n",
            "starting epoch  7 ,  5897.6572597026825\n",
            "starting epoch  8 ,  6740.112436771393\n",
            "starting epoch  9 ,  7581.810757160187\n",
            "8426.007710456848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rHlzw46OoAtG",
        "colab_type": "code",
        "outputId": "f8c560a8-00d2-4c14-9fb1-2e1804d363f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "concat_cnn = ConcatenatedCNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "            base_cnn_hyperparameters['num_features'],\n",
        "            base_cnn_hyperparameters['filter_sizes'],\n",
        "            all_models_hyperparameters['output_dim'],\n",
        "            all_models_hyperparameters['dropout']).to(device)\n",
        "concat_cnn.load_state_dict(torch.load(\"/content/gdrive/My Drive/concat-CNN-attempt-0-\"))\n",
        "# # print(\"done!\")\n",
        "\n",
        "test_model_confusion_matrix(concat_cnn, variable_length=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6721080382245967\n",
            "TPR / Recall / Sensitivity:  0.5557808112207355\n",
            "Precision:  0.7472239047042196\n",
            "F1:  0.6374385339430422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(98576.0, 25040.0, 59162.0, 74020.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "W9meBKReFDh8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 3.3 LSTM Model"
      ]
    },
    {
      "metadata": {
        "id": "nc3fwCrYFCy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConcatenatedRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, parent, text_lengths):\n",
        "        \n",
        "        text = torch.cat((parent, text), dim=0)\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        #pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        \n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "                \n",
        "        #hidden = [batch size, hid dim * num directions]\n",
        "            \n",
        "        return torch.sigmoid(self.fc(hidden.squeeze(0)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32pdk_UJFC2H",
        "colab_type": "code",
        "outputId": "56aaa812-e446-46ee-b7d6-02edd30bbd64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "tick = time.time()\n",
        "concat_rnn = ConcatenatedRNN(all_models_hyperparameters['vocabulary_size'],\n",
        "          all_models_hyperparameters['embedding_dim'], \n",
        "          two_utt_rnn_hyperparameters['hidden_size'],\n",
        "          all_models_hyperparameters['output_dim'],\n",
        "          two_utt_rnn_hyperparameters['number_of_layers'],\n",
        "          two_utt_rnn_hyperparameters['bidirectional'],\n",
        "          all_models_hyperparameters['dropout'],\n",
        "          two_utt_rnn_hyperparameters['pad_idx'])\n",
        "\n",
        "optimizer = optim.Adam(concat_rnn.parameters(), lr=two_utt_rnn_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "concat_rnn = train_model('concat-RNN-attempt-0-', concat_rnn, optimizer, criterion, variable_length=True)\n",
        "print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch  0 ,  0.02767181396484375\n",
            "starting epoch  1 ,  574.6778771877289\n",
            "starting epoch  2 ,  1149.2792840003967\n",
            "starting epoch  3 ,  1724.7828052043915\n",
            "starting epoch  4 ,  2301.061047554016\n",
            "starting epoch  5 ,  2877.241031885147\n",
            "starting epoch  6 ,  3453.98859000206\n",
            "starting epoch  7 ,  4030.646772623062\n",
            "starting epoch  8 ,  4608.009275197983\n",
            "starting epoch  9 ,  5185.02405667305\n",
            "5762.529932022095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rrnfprscFifv",
        "colab_type": "code",
        "outputId": "5d62926e-47c6-480c-9184-e1ccc8a358f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "test_model_confusion_matrix(concat_rnn, variable_length=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  0.5337502628525145\n",
            "TPR / Recall / Sensitivity:  0.253720472736556\n",
            "Precision:  0.624233355501367\n",
            "F1:  0.3607952422136092\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(103275.0, 20341.0, 99391.0, 33791.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "3bhP41mjDh7z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 4.  Modeling sarcasm by separating context"
      ]
    },
    {
      "metadata": {
        "id": "5vLkJ6QTqB4H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Utility cells for modeling"
      ]
    },
    {
      "metadata": {
        "id": "UVol7AZVqEiF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### hyperparameters\n",
        "# overall\n",
        "all_models_hyperparameters = {'embedding_dim': 100,\n",
        "                              'output_dim': 1,\n",
        "                              'dropout': 0.5,\n",
        "                              'vocabulary_size': len(TEXT.vocab),\n",
        "                              'number_of_epochs': 20,\n",
        "                              'batch_size': 50}\n",
        "\n",
        "# base lr\n",
        "# attempt 0: 0.001\n",
        "separate_lr_hyperparameters = {'learning_rate': 0.001}\n",
        "\n",
        "# base CNN\n",
        "# attempt 0: 200 x [1,2,3,4,5] x 0.0005 (bad)\n",
        "# attempt 1: 200 x [1,2,3,4,5] x 0.0001 (bad)\n",
        "# attempt 2: 50 x [3,4,5] x 0.0001 (good)\n",
        "# attempt 2.5: 80 x [3,4,5] x 0.00001 (too slow)\n",
        "# attempt 3: 80 x [3,4,5] x 0.0005 (good)\n",
        "# attempt 3: 80 x [3,4,5] x 0.0005 x 20 epochs (good)\n",
        "# jk all these are wrong\n",
        "\n",
        "# attempt 5: 20 x [2,3,4] x 0.0005 x 10 epochs (good)\n",
        "# attempt 6: \n",
        "separate_cnn_hyperparameters = {'num_features': 20,\n",
        "                            'filter_sizes': [2, 3, 4], #[2-6] didn't work\n",
        "                            'learning_rate': 0.0005}\n",
        "\n",
        "# base RNN\n",
        "# attempt 0: 15 epochs, 256 hiddens, 2 layers, dropout = 0.2, lr = 0.0005\n",
        "# attempt 1: 15 epochs, 128 hiddens, 1 layers, dropout = 0.5, lr = 0.0001 (not quite as good)\n",
        "# attempt 2: 15 epochs, 256 hiddens, 1 layers, dropout = 0.2, lr = 0.001\n",
        "separate_rnn_hyperparameters = {'hidden_size': 256,\n",
        "                            'number_of_layers': 1,\n",
        "                            'bidirectional': True,\n",
        "                            'dropout': 0.2,\n",
        "                            'pad_idx': TEXT.vocab.stoi[TEXT.pad_token],\n",
        "                            'learning_rate': 0.0001}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jw8GBSTHEXE7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 4.1 Logistic Regression Model"
      ]
    },
    {
      "metadata": {
        "id": "0vhkt0V1ELIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SeparateLogisticRegression(nn.Module):\n",
        "    def __init__(self, embedding_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "        self.embed.weight.data.copy_(vocab.vectors)\n",
        "        self.fc = nn.Linear(2 * embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, comment, parent_comment):\n",
        "        def mean_embed(text):\n",
        "            if len(text.size()) == 1:\n",
        "                text = torch.unsqueeze(text, dim=0)\n",
        "            text = text.permute(1, 0)\n",
        "            embedded = self.embed(text)\n",
        "            embedded = embedded.permute(0, 2, 1)\n",
        "            return torch.mean(embedded, dim=2)\n",
        "        avg_embedded = torch.cat((mean_embed(comment), mean_embed(parent_comment)), dim=1)\n",
        "        return torch.sigmoid(self.fc(avg_embedded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbTLAPbnm1_T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 4.2 CNN Model"
      ]
    },
    {
      "metadata": {
        "id": "Sso5cRzFnCLs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SeparateCNN1d(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_features, filter_sizes, output_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(len(vocab), embedding_dim)  # we used GLove 100-dim\n",
        "        self.embed.weight.data.copy_(vocab.vectors)\n",
        "\n",
        "        self.conv_stack_1 = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_features,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "        self.conv_stack_2 = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim,\n",
        "                      out_channels=num_features,\n",
        "                      kernel_size=fs)\n",
        "            for fs in filter_sizes\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(2 * len(filter_sizes) * num_features, output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text, parent=None):\n",
        "        def process_utterance(text, stack):\n",
        "            text = text.permute(1, 0)\n",
        "            embedded = self.embed(text)\n",
        "            embedded = embedded.permute(0, 2, 1)\n",
        "            conved = [F.relu(conv(embedded)) for conv in stack]\n",
        "            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "            cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "            return cat\n",
        "        features_from_both = torch.cat((process_utterance(parent, self.conv_stack_1),\n",
        "                                        process_utterance(text, self.conv_stack_2)),\n",
        "                                       dim=1)\n",
        "        return torch.sigmoid(self.fc(features_from_both))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D0Bs8rY3oQdO",
        "colab_type": "code",
        "outputId": "88c6b668-c01f-42cf-ad4f-ff4c2c68a695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "tick = time.time()\n",
        "separate_cnn = SeparateCNN1d(all_models_hyperparameters['embedding_dim'],\n",
        "                            separate_cnn_hyperparameters['num_features'],\n",
        "                            separate_cnn_hyperparameters['filter_sizes'],\n",
        "                            all_models_hyperparameters['output_dim'],\n",
        "                            all_models_hyperparameters['dropout']).to(device)\n",
        "\n",
        "optimizer = optim.Adam(separate_cnn.parameters(), lr=separate_cnn_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "separate_cnn = train_model('separate-CNN-attempt-5-', separate_cnn, optimizer, criterion)\n",
        "print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "starting epoch  0 ,  0.002064943313598633\n",
            "starting epoch  1 ,  570.8901927471161\n",
            "starting epoch  2 ,  1140.6789638996124\n",
            "starting epoch  3 ,  1712.6433820724487\n",
            "starting epoch  4 ,  2279.6830246448517\n",
            "starting epoch  5 ,  2836.8989083766937\n",
            "starting epoch  6 ,  3392.3847200870514\n",
            "starting epoch  7 ,  3947.432318210602\n",
            "starting epoch  8 ,  4503.103790044785\n",
            "starting epoch  9 ,  5057.863653898239\n",
            "starting epoch  10 ,  5613.037218570709\n",
            "starting epoch  11 ,  6172.709819316864\n",
            "starting epoch  12 ,  6727.041597127914\n",
            "starting epoch  13 ,  7282.446590423584\n",
            "starting epoch  14 ,  7838.030021190643\n",
            "starting epoch  15 ,  8393.186012268066\n",
            "starting epoch  16 ,  8948.123091936111\n",
            "starting epoch  17 ,  9503.272902488708\n",
            "starting epoch  18 ,  10058.326314210892\n",
            "starting epoch  19 ,  10616.52012872696\n",
            "11176.689130783081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RdIoTjVM1-S1",
        "colab_type": "code",
        "outputId": "bd8766d6-978f-42e0-d78b-fa449d22f92e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "test_model_confusion_matrix(separate_cnn, variable_length=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d96f9c7c7782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseparate_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'test_model_confusion_matrix' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "lvbk0rlgqIQ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Question 4.3 RNN Model"
      ]
    },
    {
      "metadata": {
        "id": "DzbWzPwIqHf9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SeparateRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        self.child_rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.parent_rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim * 4, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, parent, text_lengths):\n",
        "        def process_utterance(text, rnn):\n",
        "            embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "            #pack sequence\n",
        "            packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "            packed_output, (hidden, cell) = rnn(packed_embedded)\n",
        "\n",
        "            #unpack sequence\n",
        "            output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
        "            return cat\n",
        "        features_from_both = torch.cat((process_utterance(parent, self.parent_rnn),\n",
        "                                        process_utterance(text, self.child_rnn)),\n",
        "                                       dim=1)\n",
        "        return torch.sigmoid(self.fc(features_from_both))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ytPEKoShq34C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tick = time.time()\n",
        "separate_rnn = SeparateRNN(all_models_hyperparameters['vocabulary_size'],\n",
        "          all_models_hyperparameters['embedding_dim'], \n",
        "          two_utt_rnn_hyperparameters['hidden_size'],\n",
        "          all_models_hyperparameters['output_dim'],\n",
        "          two_utt_rnn_hyperparameters['number_of_layers'],\n",
        "          two_utt_rnn_hyperparameters['bidirectional'],\n",
        "          all_models_hyperparameters['dropout'],\n",
        "          two_utt_rnn_hyperparameters['pad_idx'])\n",
        "\n",
        "optimizer = optim.Adam(separate_rnn.parameters(), lr=two_utt_rnn_hyperparameters['learning_rate'])\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "concat_rnn = train_model('separate-RNN-attempt-0-', separate_rnn, optimizer, criterion, variable_length=True)\n",
        "print(time.time() - tick)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lvBgAW61q4M-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}